{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "## Covers\n",
    "   > Character level Vanilla RNN implementation.     \n",
    "    Generating a sequence from a random char picked from the test set.  \n",
    "    Experiments to establish the roles the state variables play.\n",
    "   \n",
    " <img src='assets/rnn.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "TF version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "print('Python version:',sys.version)\n",
    "print('TF version:',tf.__version__)\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='assets/input.png'>\n",
    "\n",
    "> The above toy example was chosen as input to check whether the network can figure out whether the singer is a he/she based on the context. The n/w should remember if it has seen either bob or julia before and it predicts 'he sings' or 'she sings' appropriately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a test data\n",
    "#Change the input data to play around\n",
    "text = 'Bob is a singer, he sings well. Julia is also a singer, she sings better '\n",
    "x = list(text[0:len(text)-1])\n",
    "\n",
    "#y is the next char of x\n",
    "y = list(text[1:len(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Input :x \n",
      " ['B', 'o', 'b', ' ', 'i', 's', ' ', 'a', ' ', 's', 'i', 'n', 'g', 'e', 'r', ',', ' ', 'h', 'e', ' ', 's', 'i', 'n', 'g', 's', ' ', 'w', 'e', 'l', 'l', '.', ' ', 'J', 'u', 'l', 'i', 'a', ' ', 'i', 's', ' ', 'a', 'l', 's', 'o', ' ', 'a', ' ', 's', 'i', 'n', 'g', 'e', 'r', ',', ' ', 's', 'h', 'e', ' ', 's', 'i', 'n', 'g', 's', ' ', 'b', 'e', 't', 't', 'e', 'r']\n",
      "\n",
      "**LabelEncoder :x \n",
      " [ 3 13  6  0 10 15  0  5  0 15 10 12  8  7 14  1  0  9  7  0 15 10 12  8\n",
      " 15  0 18  7 11 11  2  0  4 17 11 10  5  0 10 15  0  5 11 15 13  0  5  0\n",
      " 15 10 12  8  7 14  1  0 15  9  7  0 15 10 12  8 15  0  6  7 16 16  7 14]\n",
      "\n",
      "**LabelBinarizer :x \n",
      " [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "\n",
      "**Transform back :x \n",
      " ['B' 'o' 'b' ' ' 'i' 's' ' ' 'a' ' ' 's' 'i' 'n' 'g' 'e' 'r' ',' ' ' 'h'\n",
      " 'e' ' ' 's' 'i' 'n' 'g' 's' ' ' 'w' 'e' 'l' 'l' '.' ' ' 'J' 'u' 'l' 'i'\n",
      " 'a' ' ' 'i' 's' ' ' 'a' 'l' 's' 'o' ' ' 'a' ' ' 's' 'i' 'n' 'g' 'e' 'r'\n",
      " ',' ' ' 's' 'h' 'e' ' ' 's' 'i' 'n' 'g' 's' ' ' 'b' 'e' 't' 't' 'e' 'r']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#Converting the input to one hot encoding \n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer\n",
    "\n",
    "#String to int\n",
    "le = LabelEncoder()\n",
    "vocab = list(set(text))\n",
    "le.fit(vocab)\n",
    "text_int = le.transform(vocab)\n",
    "print('**Input :x \\n', x)\n",
    "x = le.transform(x)\n",
    "y = le.transform(y)\n",
    "print('\\n**LabelEncoder :x \\n', x)\n",
    "#One-hot encoding\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(text_int)\n",
    "x = lb.transform(x)\n",
    "y = lb.transform(y)\n",
    "print('\\n**LabelBinarizer :x \\n', x)\n",
    "print('\\n**Transform back :x \\n', le.inverse_transform(lb.inverse_transform(x)))\n",
    "vocab_dict = dict(zip(text_int.tolist(), vocab))\n",
    "vocab_reverse_dict = dict(zip(vocab, text_int.tolist()))\n",
    "vocab_onehot = lb.transform(text_int)\n",
    "vocab_onehot_dict = dict(zip(text_int.tolist(), vocab_onehot))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a char RNN\n",
    "\n",
    " >The RNN has number of states = 'n_state_size' and the number hidden units = hidden_size   \n",
    " The inputs X, Y and fed in the size = {length of each one_hot_encoded input, no of states}    \n",
    " The RNN class has the following methods:   \n",
    "     * init = Initializes the weights, state variables, loss function and optimizers. Creates the state graph.    \n",
    "     * train = Trains the classifier given the inputs X, Y and the 'valid_char' string for testing during training\n",
    "     * test = generates a sequence based on a input test string (of length state_size)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class charRNN(object):\n",
    "    \n",
    "    def __init__(self, n_hidden, n_vocab_size, n_state_size, vocab_dict, vocab_reverse_dict, vocab_onehot_dict):\n",
    "        \n",
    "        self.hidden_size = n_hidden\n",
    "        self.state_size = n_state_size\n",
    "        self.truncated_backprop_length = n_state_size\n",
    "        self.vocab_size = n_vocab_size\n",
    "        \n",
    "        #copy the dictionary\n",
    "        self.vocab_onehot_dict = vocab_onehot_dict\n",
    "        self.vocab_reverse_dict = vocab_reverse_dict\n",
    "        self.vocab_dict = vocab_dict\n",
    "               \n",
    "        # model inputs\n",
    "        self.x = tf.placeholder(tf.float32, [self.vocab_size, self.truncated_backprop_length])\n",
    "        self.y = tf.placeholder(tf.int32, [self.vocab_size, self.truncated_backprop_length])\n",
    "        \n",
    "        inputs_series = tf.unstack(self.x, axis=1) # unpack the columns\n",
    "        labels_series = tf.unstack(self.y, axis=1) # unpack the columns\n",
    "        \n",
    "        # get the network weights\n",
    "        network_weights = self._initialize_weights()\n",
    "        self.weights = network_weights\n",
    "        \n",
    "        # model\n",
    "        current_state = self.weights['init_state']\n",
    "        states_series = []\n",
    "        for X_i in inputs_series:\n",
    "            X_i = tf.reshape(X_i, [-1, self.vocab_size])       \n",
    "            next_state = tf.tanh(tf.matmul(X_i, self.weights['Wxh']) + \n",
    "                                 tf.matmul(current_state, self.weights['Whh']) + self.weights['bhh'])           \n",
    "            states_series.append(next_state)\n",
    "            current_state = next_state\n",
    "        self.states_series = states_series\n",
    "        \n",
    "        #logits and cost\n",
    "        logits_series = [tf.matmul(state, self.weights['Why'])+ self.weights['bhy'] for state in self.states_series]\n",
    "        \n",
    "        self.pred_series = [tf.nn.softmax(logits) for logits in logits_series]\n",
    "        self.loss = tf.reduce_mean([tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels) for logits, labels in zip(logits_series, labels_series)])\n",
    "        self.optimizer = tf.train.AdagradOptimizer(0.05).minimize(self.loss)\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "    \n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        all_weights = dict()\n",
    "        all_weights['init_state'] = tf.Variable(tf.zeros([1, self.hidden_size]), dtype=tf.float32, name='init_state')\n",
    "        all_weights['Wxh'] = tf.Variable(tf.truncated_normal((self.vocab_size, self.hidden_size), stddev=0.01, seed=SEED), \n",
    "                                         dtype=tf.float32, name='Wxh')\n",
    "        all_weights['Whh'] = tf.Variable(tf.truncated_normal((self.hidden_size, self.hidden_size), stddev=0.01, seed=SEED), \n",
    "                                         dtype=tf.float32, name='Whh')\n",
    "        all_weights['bhh'] = tf.Variable(tf.zeros(self.hidden_size), dtype=tf.float32, name='bhh')\n",
    "        all_weights['Why'] = tf.Variable(tf.truncated_normal((self.hidden_size, self.vocab_size), stddev=0.01, seed=SEED), \n",
    "                                         dtype=tf.float32, name='Why')\n",
    "        all_weights['bhy'] = tf.Variable(tf.zeros(self.vocab_size), dtype=tf.float32, name='bhy')\n",
    "        \n",
    "        return all_weights\n",
    "    \n",
    "    def train(self, X, Y, n_epoch, valid_char):\n",
    "        epoch_disp = 2\n",
    "        for epoch in range(n_epoch):\n",
    "            for batch in range(len(X)):\n",
    "                start_idx = batch\n",
    "                end_idx = min(start_idx + self.truncated_backprop_length, len(x))\n",
    "                if (end_idx - start_idx) < self.truncated_backprop_length :\n",
    "                    break\n",
    "                batchX = X[start_idx:end_idx].T\n",
    "                batchY = Y[start_idx:end_idx].T\n",
    "                loss, _ = self.sess.run((self.loss, self.optimizer), feed_dict={self.x: batchX, self.y: batchY})\n",
    "            \n",
    "            if(epoch %(epoch_disp)**2 ==0):\n",
    "                print('Loss epoch({:})={:3.4f}'.format(epoch,loss))\n",
    "                input_char = valid_char\n",
    "                print([input_char], end=' ')\n",
    "                batchX = np.array([self.vocab_onehot_dict[self.vocab_reverse_dict[i]].tolist() for i in input_char])\n",
    "                self.generate_sequence(batchX)\n",
    "                epoch_disp += 2\n",
    "            \n",
    "    def test(self, input_char):\n",
    "        print([input_char], end=' ')\n",
    "        batchX = np.array([self.vocab_onehot_dict[self.vocab_reverse_dict[i]].tolist() for i in input_char])\n",
    "        self.generate_sequence(batchX, 55)\n",
    "    \n",
    "    def generate_sequence(self, X, n_samples=50):\n",
    "        for i in range(n_samples):\n",
    "            pred_series = self.sess.run(self.pred_series, feed_dict={self.x: X.T})\n",
    "            pred_idx = np.argmax(pred_series, axis=2).flatten()\n",
    "            X = np.roll(X, -1, axis=0)\n",
    "            X[self.truncated_backprop_length-1] = self.vocab_onehot_dict[pred_idx[self.truncated_backprop_length-1]]\n",
    "            print(self.vocab_dict[pred_idx[self.truncated_backprop_length-1]], end='')\n",
    "        print('\\n')\n",
    "    \n",
    "    def save_model(self, model_path):\n",
    "        saver = tf.train.Saver()\n",
    "        save_path = saver.save(self.sess, model_path)\n",
    "        print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The network is trained below, and during testing, the output which gives the max probabilty is feed back to the input to generate the next character and then its repeated recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-94304a87ed76>:42: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Loss epoch(0)=2.7951\n",
      "['Bob is a singer, he ']                                                   \n",
      "\n",
      "Loss epoch(16)=0.6482\n",
      "['Bob is a singer, he '] sings well. Julia is also a singer, she sings well\n",
      "\n",
      "Loss epoch(36)=0.1379\n",
      "['Bob is a singer, he '] sings better also Julia is ie se Julia is also a s\n",
      "\n",
      "Loss epoch(64)=0.0767\n",
      "['Bob is a singer, he '] sings well. Julia is also a singer, she sings bett\n",
      "\n",
      "Model saved in file: ./char_rnn\n"
     ]
    }
   ],
   "source": [
    "# training the Session\n",
    "tf.reset_default_graph()\n",
    "n_hidden = 50\n",
    "n_vocab_size = len(vocab)\n",
    "n_state_size = 20\n",
    "\n",
    "rnn = charRNN(n_hidden, n_vocab_size, n_state_size, vocab_dict, vocab_reverse_dict, vocab_onehot_dict)\n",
    "\n",
    "\n",
    "rnn.train(x, y, 100, 'Bob is a singer, he ')\n",
    "\n",
    "save_path = './char_rnn'\n",
    "rnn.save_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > The training starts with bad predictions but as time progresses it prediction becomes better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bob is a singer, he '] sings well. Julia is also a singer, she sings better a \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# testing on a input\n",
    "rnn.test('Bob is a singer, he ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments with state variables\n",
    "> The hidden state $h^{(t)} = g(W_{xh}*x^{(t)} + W_{hh}*h^{(t-1)} + b_{hh}) $  can be rewritten as $h^{(t)} = g(W_{h} * [\\begin{array}{cccc} x^{(t)} \\\\ h^{(t-1)} \\end{array}] + b_{hh}) $. The state equation can be simplified to a single weight with the concatenation of the previous state and the input as the actual input to a feed forward layer.\n",
    "<img src='assets/input_concat.png'>\n",
    "> The state variables encode information of the inputs the network has seen before.\n",
    "> We will reset some of the values to zero, to see how it affects the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "\n",
    "def pred_next_char(input_char, sess, state, weights, mask_state_var):\n",
    "    X = np.array(vocab_onehot_dict[vocab_reverse_dict[input_char]].tolist())\n",
    "    X = tf.constant(X, dtype=tf.float32)\n",
    "    X = tf.reshape(X, [-1, n_vocab_size])\n",
    "    mask = tf.constant(mask_state_var, dtype=tf.float32)\n",
    "    state = tf.multiply(state, mask)\n",
    "    h = tf.tanh(tf.matmul(X, weights['Wxh']) + tf.matmul(state, weights['Whh']) + weights['bhh'])\n",
    "    p = tf.matmul(h, weights['Why'])+weights['bhy']\n",
    "    out = sess.run(p)\n",
    "    return out, h\n",
    "\n",
    "def predict_var_char(input_char, mask_state_var):\n",
    "    tf.reset_default_graph()\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_path+ '.meta')\n",
    "        loader.restore(sess, save_path)\n",
    "        loaded_weights = dict()\n",
    "        loaded_init_state = loaded_graph.get_tensor_by_name('init_state:0')\n",
    "        loaded_weights['Wxh'] = loaded_graph.get_tensor_by_name('Wxh:0')\n",
    "        loaded_weights['Whh'] = loaded_graph.get_tensor_by_name('Whh:0')\n",
    "        loaded_weights['bhh'] = loaded_graph.get_tensor_by_name('bhh:0')\n",
    "        loaded_weights['Why'] = loaded_graph.get_tensor_by_name('Why:0')\n",
    "        loaded_weights['bhy'] = loaded_graph.get_tensor_by_name('bhy:0')\n",
    "    \n",
    "        #input_char = 'do'\n",
    "        print('In:', [input_char], end=' ')\n",
    "        print('Out: ', end='')\n",
    "        \n",
    "        #h = tf.Variable(tf.zeros([1, n_hidden]), dtype=tf.float32)\n",
    "        #new_var = tf.variables_initializer([h])\n",
    "        #sess.run(new_var)\n",
    "        \n",
    "        h = loaded_init_state\n",
    "        \n",
    "        for i in range(55):\n",
    "            for ch in input_char:\n",
    "                out, h = pred_next_char(ch, sess, h, loaded_weights, mask_state_var)  \n",
    "            idx_o = np.argmax(out, axis=1)\n",
    "            input_char = vocab_dict[idx_o[0]]\n",
    "            print(input_char, end='')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Testing whether the loaded model works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./char_rnn\n",
      "In: ['Bob is a'] Out:  singer, he sings well. Julia is also a singer, she sin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict_var_char('Bob is a',[1 for i in range(n_hidden)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Setting some random values of the state variable as zero, to notice the effect it has on the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./char_rnn\n",
      "In: ['Bob is a '] Out: singer, she sings better a singer, she sings better a s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask = [1 for i in range(n_hidden)]\n",
    "#for i in range(n_hidden):\n",
    "mask[7] = 0 # The first hidden unit is reset to zero\n",
    "predict_var_char('Bob is a ',mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The above example sets a value the state variable to zero, we see that the he/she prediction is lost by the network. So we can say that this value had the info about when to switch to he/she based on the context. This operation can be thought of as a forget gate like in LSTM where the he info is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-- EOF --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
