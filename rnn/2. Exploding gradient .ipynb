{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________\n",
    "# Vanishing/Exploding gradient problem in rnn\n",
    "__________________________\n",
    "Lets analyse the gradients of a simple rnn to understand the problem we encounter during training.\n",
    "\n",
    "<img src='assets/vanish_grad1.png'>\n",
    "\n",
    "RNNs are trained using 'Backpropagation through time (BPTT)', where the objective is to learn the parameters $W_{xh}, W_{hh}, W_{hy}$ by obtaining the error gradients with respect to those parameters through gradient desent.\n",
    "\n",
    "Lets compute the gradient $\\large\\frac{\\partial E} {\\partial W}$ for the above RNN which has three stages. The weights are obtained by summing up the gradients obtained at each time step: $\\sum\\limits_{t}\\large\\frac{\\partial E_{t}} {\\partial W_{hh}}$. But lets only calculate $\\large\\frac{\\partial E_{t+1}} {\\partial W}$ for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets do the 'computational graph gradient flow approach' to track the gradients as it flows back in various computational nodes. Go through [Gradient flow in neural network lesson]('http://localhost:8888/notebooks/intro/Gradient%20Flow.ipynb') for the details.\n",
    "\n",
    "The equations involved:   \n",
    "$$\\begin{align}\n",
    "z_{t} &= U x_{t} + W s_{t-1} \\\\    \n",
    "s_{t} &= tanh(z_{t}) \\\\    \n",
    "q_{t} &= V s_{t} \\\\    \n",
    "o_{t} &= softmax(q_{t}) \\\\   \n",
    "\\end{align} $$\n",
    "\n",
    "\n",
    "#### Backpropagating from the last output node:\n",
    "* Since we need to calculate $E_{t+1}$ with respect to $W$, we will start with output node: $o_{t+1}$. Since the weights W is common for all time steps, we need to individualy calculate their gradient for each time step and add the results for the final gradient. Note all the notations below are in matrix form.\n",
    "* ##### W gradient at time (t+1):\n",
    "    + The error gradient $\\large\\frac{\\partial E_{t+1}} {\\partial o_{t+1}}$ is propagated back.\n",
    "    + Next is the activation gate, with output $o_{t+1}$ and input $q_{t+1}$ , so the local gradient $\\large\\frac{\\partial o_{t+1}} {\\partial q_{t+1}}$ gets multiplied with the incoming gradient. Thus we get\n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} $$\n",
    "        \n",
    "    + Now we encounter the multiplication gate with weights V and $s_{t+1}$, so the gradient towards $s_{t+1}$ is the product of the incoming gradient multiplied by V. The gradient at this point is \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \\small V $$\n",
    "    \n",
    "    + After that comes the activation unit whose output is $s_{t+1}$ and input is $z_{t+1}$. So the local gradient gets multipled with the incoming gradient. And we get,\n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \\small V . \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} $$\n",
    "    \n",
    "    + Next comes the multiplication gate where the gradient at W is obtained by multiplying the incoming gradient with the 'other input' $s_{t}$, while the gradient that flows through $s_{t}$ gets multiplied with W. So the gradient at W is:   \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial W} = \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    S_{t}$$ \n",
    "    \n",
    "    and the other gradient that flows through $s_{t+1}$ is   \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W$$\n",
    "\n",
    "* ##### W gradient at time (t):\n",
    "    + Here the next node is the activation unit, so the local gradient gets multiplied with the incoming gradient from the previous step. We get, \n",
    "    \n",
    "    $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}$$\n",
    "    \n",
    "    + Next comes the multiplication gate where the gradient at W is obtained by multiplying the incoming gradient with the 'other input' $s_{t-1}$, while the gradient that flows through $s_{t-1}$ gets multiplied with W. So the gradient at W is:\n",
    "    \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial W} =\n",
    "     \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-1}$$ \n",
    "     and the other gradient that flows through $s_{t-1}$ is \n",
    "     \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W$$ \n",
    "\n",
    "* ##### W gradient at time (t-1):\n",
    "    + Repeating the same as above, we get the gradient at W as:\n",
    "    \n",
    "     $$\\frac{\\partial E_{t+1}} {\\partial W} =\n",
    "     \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W. \\frac{\\partial s_{t-1}} {\\partial z_{t-1}}. S_{t-2}$$ \n",
    "    \n",
    "So the final gradient is \n",
    "\n",
    "$$\\begin{align}\\frac{\\partial E_{t+1}} {\\partial W} &= \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    S_{t}  \\\\ &+ \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. S_{t-1} \n",
    "    \\\\ &+ \n",
    "    \\frac{\\partial E_{t+1}} {\\partial o_{t+1}} . \n",
    "    \\frac{\\partial o_{t+1}} {\\partial q_{t+1}} . \n",
    "    \\small V . \n",
    "    \\frac{\\partial s_{t+1}} {\\partial z_{t+1}} . \n",
    "    \\small W . \n",
    "    \\frac{\\partial s_{t}} {\\partial z_{t}}. W. \\frac{\\partial s_{t-1}} {\\partial z_{t-1}}. S_{t-2} \\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the terms $\\frac{\\partial s_{t-n}} {\\partial z_{t-n}}$, they are multiplied a lot and they are derivatives of the activation function. If the activation function is sigmoid, then its derivative will always be less than 0.25, and multiplying a lot yields a really small number, which is causes vanishing gradient problem!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does LSTM prevent the vanishing gradient problem?\n",
    "\n",
    "We have seen above that the problem is primarily because of the deriviative of the activation function. In the case of LSTM, the activation is the identity function with a derivative of 1.0. So the backpropagated gradient neither vanishes or explodes when passing through. The details will be published in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- EOF --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
