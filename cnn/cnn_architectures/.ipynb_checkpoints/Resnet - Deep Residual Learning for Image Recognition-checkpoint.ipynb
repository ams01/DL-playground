{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________\n",
    "\n",
    "# Deep Residual Learning for Image Recognition\n",
    "\n",
    "https://arxiv.org/pdf/1512.03385.pdf\n",
    "\n",
    "Paper summary by: vijay Mariappan\n",
    "_____________________\n",
    "*Deep residual nets swept all detection and classification competitions in ILSVRC & COCO 2015.* The network that they used had 152 layers, an impressive 8 times deeper than a comparable VGG network\n",
    "\n",
    "<img src='ILSVRC.png'>\n",
    "_________________\n",
    "\n",
    "### Claims:\n",
    "\n",
    "> **Deeper 'plain' neural networks are more difficult to train.**\n",
    "\n",
    "<img src='train_error.png'>\n",
    "\n",
    "The above represents the training and test error of a large 'plain' neural network. The surpising part is not that the test error is large for the 56-layer network compared to a 20-layer network, but the training error is also large compared to the smaller network.\n",
    "\n",
    "Doesnt large networks overfit the data, and so we should get a lower training error than a smaller network?. Or is it a problem because of the notorious vanishing/exploding gradients?.The authors claim that this behaviour is because of 'optimization problem': Deeper models are difficult to optimize. They construct an example to show that.\n",
    "\n",
    "### An experiment\n",
    "\n",
    "\n",
    "<img src='shallower.png'>\n",
    "\n",
    "In the above experiment:\n",
    "\n",
    " * The deeper model is constructed by coping the original shallow network and adding 'extra layers' which are set as 'identity' (where these layers output the input as it is).  So this should produce no higher training error than its shallower counterpart, right?\n",
    " * No, the error is higher and the current solvers (like SGD or any momentum based) are unable to handle it.\n",
    " * The degradation problem suggests that the solvers might have difficulties in approximating identity mappings by multiple nonlinear layers.\n",
    " ______________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addressing the above degradation problem using deep residual framework\n",
    "* Hypothesis: It is easier to optimize theresidual mapping than to optimize the original, unreferenced mapping.\n",
    "* Lets say the desired underying mapping is $H(x)$,they let the stack non-linear layer fit another mapping: $F(x):= H(x) -x$. The original mapping is recast into $F(x) + x$\n",
    "* The formulation of $F(x) + x$ can be realized by feedforward neural networks with “shortcut connections”.\n",
    "* So solvers need to learn residual functions i.e. F(x) = H(x) - x. \n",
    "* With the residual learning reformulation, if identity mappings are optimal, the solvers may simply drive the weights of the multiple nonlinear layers toward zero to approach identity mappings.\n",
    "* In real cases, it is unlikely that identity mappings are optimal, but the reformulation may help to precondition the problem.\n",
    "\n",
    "<img src='resnet_new.png'>\n",
    "\n",
    "Note: The desired mapping is still $H(x)$, but the solvers need to learn only $F(x)$ as the shortcut connections don't require additional parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets summarize till this point\n",
    "* The authors think that the current solvers and architecture have a hard time learning the identity function for certain layers of the net, and may have an easier time learning zero mappings for those same layers. \n",
    "* Ultimately, this motivates their setup for learning small residuals and adding them to the input, rather than just transforming the whole input directly.\n",
    "* Each subsequent layer is only responsible for, in effect, fine tuning the output from a previous layer by just adding a learned \"residual\" to the input. This differs from a more traditional approach where each layer has to generate the whole desired output.\n",
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Design\n",
    "\n",
    "<img src='plainvsRes_v2.png'>\n",
    "\n",
    "**Full ResNet architecture:**\n",
    "* Stack residual blocks\n",
    "\n",
    "* Every residual block has two 3x3 conv layers\n",
    "\n",
    "* Periodically, double # of filters and downsample spatially using stride 2.\n",
    "\n",
    "<img src='experiments.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='practical-design.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the complexity of the above\n",
    "\n",
    "### left size. \n",
    "  * The input is `32*32*64` and the padding is SAME\n",
    "  * for 1st conv 3x3, 64 : number of input channels is 64 for each output pixel the computation would be `3x3x64`. Since the output is `32*32*64`, the number of multiplications are `32*32*64*3*3*64`\n",
    "  * for 2nd conv 3x3, 64: its the same as above as the input and output channels are 64: `32*32*64*3*3*64` \n",
    "  \n",
    "  **Total = 75M**\n",
    "  \n",
    "### On the right side:\n",
    "  * The input is `32*32*256` and the padding is SAME\n",
    "  * for 1st 1x1 conv, 64: number of input channels is 256, so for each output pixel the computation would be `1x1*256`. Since the output is `32*32*64`, the total mumber of multiplications are `32*32*64*1*1*256`.\n",
    "  * for 2nd conv 3x3, 64: its going to be `32*32*64*3*3*64` as calculated previously.\n",
    "  * for 3rd conv 1x1, 256, the number of imput channels is 64, for each output pixel the computation would be `1*1*64`. Since the output is `32*32*256`, the total number of multiplciations are `32*32*256*1*1*64`\n",
    "  \n",
    "  **Total = 71M**\n",
    "  \n",
    "Consider the case where the input is of size `32*32*256` for the right side of the network, the total computation would be `32*32*64*3*3*256*2` = 300 M, very expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentry\n",
    "One of the usefulness of skip connections are the easiness in which the gradients flow. Lets calculate for one block:\n",
    "\n",
    "<img src='gradients.png'>\n",
    "\n",
    "Now if you consider multiple of them stacked, the gradients would be of the form: $\\frac{\\partial E}{\\partial y}*(1 + F'_{1})*(1 + F'_{2})...(1 + F'_{N})$ without the skip connection it would be  $\\frac{\\partial E}{\\partial y}*(F'_{1})*(F'_{2})...(F'_{N})$. So if the magnitude of gradients are < 1, which is most of the cases, as $F'(x)$ corresponds to $weights_1*weights_2$, and they are generally initialized between ${-1, 1}$ then we may have a problem of vanishing gradients, that is avoided in the case of skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "* A `224×224` crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted.\n",
    "* Batch normalization(BN) is applied  right  after  each  convolution  and before activation.\n",
    "* ReLu is the activation thats been used\n",
    "\n",
    "<img src='block.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "print(tf.__version__)\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple experiment to prove that the gradients flow well in a resnet model vs plain vanilla model\n",
    "\n",
    "* Create two models one with skip connection and one without\n",
    "* Removed BN for simplicity.\n",
    "* The block is repeated 50 times to construct a deep model.\n",
    "* Measure the gradient flow across the architectures by finding the gradient from the output with respect to the input.\n",
    "* Resnet should have a higher L2 norm of the gradients compared with the plain network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input used is 224x224\n",
    "input = tf.placeholder(tf.float32,(1, 224, 224, 3))\n",
    "\n",
    "# if skip connection is False, then its plain network\n",
    "def resnet_block(input_layer, output_channel, first_block=False, skip_connection=True):\n",
    "    \n",
    "    if first_block:\n",
    "        conv =  tf.layers.conv2d(input_layer, output_channel, (7,7), strides=(2,2), padding='SAME')\n",
    "        output = tf.layers.average_pooling2d(conv,(2,2), (2,2), padding='VALID')\n",
    "        \n",
    "    else:    \n",
    "        \n",
    "        # Removed batch norm for simplicity\n",
    "        conv =  tf.layers.conv2d(input_layer, output_channel, (3,3), strides=(1,1), padding='SAME',activation=tf.nn.relu)\n",
    "    \n",
    "        # relu activation\n",
    "        act = tf.nn.relu(conv)\n",
    "    \n",
    "        # \n",
    "        conv =  tf.layers.conv2d(act, output_channel, (3,3), strides=(1,1), padding='SAME')\n",
    "        \n",
    "        if skip_connection:\n",
    "            output = tf.nn.relu(conv + input_layer)\n",
    "        else:\n",
    "            output = tf.nn.relu(conv)\n",
    "    \n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 50 resnet_blocks and plain blocks\n",
    "input_layer_res = input\n",
    "input_layer_plain = input\n",
    "is_first = True\n",
    "for _ in range(50):\n",
    "    output_resnet = resnet_block(input_layer_res, 64, first_block=is_first)\n",
    "    output_plain = resnet_block(input_layer_plain, 64, first_block=is_first,skip_connection=False)\n",
    "    is_first = False\n",
    "    input_layer_res = output_resnet\n",
    "    input_layer_plain = output_plain\n",
    "t_grad_resnet = tf.gradients(output_resnet, input)[0]\n",
    "t_grad_plain = tf.gradients(output_plain, input)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(10):\n",
    "        g_resnet, g_plain = sess.run([t_grad_resnet, t_grad_plain], {input: np.random.normal(size=(1,224,224,3))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet case l2 sum of gradients: 407905.56\n",
      "Plain Vanilla, l2 sum of gradients: 2.4008885e-13\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as LA\n",
    "print('Resnet case l2 norm:',LA.norm(g_resnet))\n",
    "print('Plain Vanilla, l2 norm:',LA.norm(g_plain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resnet indeed has better gradient flow.**\n",
    "\n",
    "- EOF -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
