{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet implementation with MNIST example\n",
    "\n",
    "### The paper is described in https://arxiv.org/abs/1412.6622\n",
    "\n",
    "<img src='assets/triplet.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n",
      "TF version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data # MNIST data\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "print('Python version:',sys.version)\n",
    "print('TF version:',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "LOGDIR = 'logs/'\n",
    "# Download the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data('/home/vijay/datasets/MNIST_data/mnist.npz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a CNN model\n",
    "\n",
    "# Hyper parameters\n",
    "DROP_OUT = 0.5\n",
    "LEARNING_RATE = 3e-4\n",
    "\n",
    "def network(x, is_train):\n",
    "    \n",
    "    # convolutional layer\n",
    "    net = tf.layers.conv2d(x, 96, kernel_size=(3,3), strides=(1,1), name='conv1', activation=tf.nn.relu)\n",
    "    \n",
    "    # maxpool layer\n",
    "    net = tf.layers.max_pooling2d(net, pool_size=(2, 2), strides=(2, 2), name='pool')\n",
    "    \n",
    "    # add regularization\n",
    "    net = tf.layers.dropout(net, rate=DROP_OUT, training=is_train)\n",
    "    \n",
    "    # flatten the layers    \n",
    "    net = tf.layers.flatten(net)\n",
    "    \n",
    "    # fully connected layer\n",
    "    net = tf.layers.dense(net, 1024, activation=tf.nn.relu, name='fc1')\n",
    "    \n",
    "    # add regularization\n",
    "    net = tf.layers.dropout(net, rate=DROP_OUT, training=is_train)\n",
    "    \n",
    "    # fully connected layer\n",
    "    net = tf.layers.dense(net, 256, activation=tf.nn.relu, name='fc2')   \n",
    "    \n",
    "    # Normalize output\n",
    "    net = tf.nn.l2_normalize(net, axis=1, epsilon=1e-10, name='l2-norm')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# placeholder for inputs\n",
    "im_1 = tf.placeholder(tf.float32, [None, 28, 28, 1], name='img1')\n",
    "im_2 = tf.placeholder(tf.float32, [None, 28, 28, 1], name='img2')\n",
    "labels = tf.placeholder(tf.float32, [None, 1], name='labels')\n",
    "is_train = tf.placeholder(tf.bool)\n",
    "margin = 1.0\n",
    "\n",
    "#model with shared weights\n",
    "with tf.variable_scope('Triplet') as scope:\n",
    "    model1_embed = network(im_1, is_train)\n",
    "    scope.reuse_variables()\n",
    "    model2_embed = network(im_2, is_train)\n",
    "    \n",
    "d2 = tf.reduce_sum(tf.square(tf.subtract(model1_embed, model2_embed)), 1,  keepdims=True)\n",
    "distance = tf.sqrt(d2)\n",
    "C = tf.constant(margin, name='C')\n",
    "neg = tf.multiply(tf.subtract(1.0, labels) , tf.square(tf.maximum(0., tf.subtract(C, distance))))\n",
    "pos = tf.multiply(labels, d2)\n",
    "losses = tf.add(pos, neg)\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(losses)\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "summ = tf.summary.merge_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train step: epoch [0/30]: loss: 0.192\n",
      "Train step: epoch [1/30]: loss: 0.119\n",
      "Train step: epoch [2/30]: loss: 0.074\n",
      "Train step: epoch [3/30]: loss: 0.052\n",
      "Train step: epoch [4/30]: loss: 0.041\n",
      "Train step: epoch [5/30]: loss: 0.033\n",
      "Train step: epoch [6/30]: loss: 0.027\n",
      "Train step: epoch [7/30]: loss: 0.023\n",
      "Train step: epoch [8/30]: loss: 0.019\n",
      "Train step: epoch [9/30]: loss: 0.015\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-57eec0b0a634>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m                                                     \u001b[0mim_2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                                                     \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                                                     is_train: False})\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtest_batch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1120\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \"\"\"\n\u001b[1;32m    351\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 242\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "import time\n",
    "import random\n",
    "def compute_accuracy(prediction,labels):\n",
    "    \n",
    "    return np.mean(labels.ravel() == 1*(predict.ravel() < 0.5))\n",
    "\n",
    "def create_pairs(x, class_idx):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n_class = len(class_idx)\n",
    "    n = min([len(class_idx[d]) for d in range(n_class)]) - 1\n",
    "    for d in range(n_class):\n",
    "        for i in range(n):\n",
    "            idx1, idx2 = class_idx[d][i], class_idx[d][i+1]\n",
    "            pairs += [[x[idx1], x[idx2]]]\n",
    "            rd = random.randrange(1, n_class)\n",
    "            dn = (d + rd) % n_class\n",
    "            idx1, idx2 = class_idx[d][i], class_idx[dn][i]\n",
    "            pairs += [[x[idx1], x[idx2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "    \n",
    "def next_batch(s,e,inputs,labels):\n",
    "    input1 = inputs[s:e,0]\n",
    "    input2 = inputs[s:e,1]\n",
    "    y= np.reshape(labels[s:e],(len(range(s,e)),1))\n",
    "    return input1,input2,y\n",
    "    \n",
    "batch_size = 512\n",
    "n_epoch = 30\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#write the summaries\n",
    "writer = tf.summary.FileWriter(LOGDIR + 'triplet')\n",
    "writer.add_graph(sess.graph) \n",
    "\n",
    "\n",
    "#create training/test positive-negative pairs\n",
    "n_classes = 10\n",
    "class_idx = [np.where(y_train == i)[0] for i in range(n_classes)]\n",
    "train_pairs, train_labels = create_pairs(X_train, class_idx)\n",
    "\n",
    "class_idx = [np.where(y_test == i)[0] for i in range(n_classes)]\n",
    "test_pairs, test_labels = create_pairs(X_test, class_idx)\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    avg_loss = 0.\n",
    "    avg_acc = 0.\n",
    "    start_time = time.time()\n",
    "    train_batch = int(len(y_train)/batch_size)\n",
    "    test_batch = int(len(y_test)/batch_size)\n",
    "    \n",
    "    for step in range(train_batch):\n",
    "\n",
    "        batch_x1, batch_x2, batch_y =next_batch(step*batch_size, (step+1)*batch_size, train_pairs, train_labels)\n",
    "        _, loss_b = sess.run([optimizer, loss], feed_dict={\n",
    "                                            im_1: batch_x1[...,None],\n",
    "                                            im_2: batch_x2[...,None],\n",
    "                                            labels: batch_y,\n",
    "                                            is_train: True})\n",
    "        avg_loss += loss_b \n",
    "        if ((step % 100) == 0) & (step > 0):\n",
    "            print ('Train step: epoch [%d/%d]: loss: %.3f' % (epoch, n_epoch, avg_loss/(step+1)))\n",
    "    # Validation\n",
    "    if ((epoch+1) % 10) == 0:\n",
    "        acc_avg = 0.\n",
    "        loss_avg = 0.\n",
    "        for step in range(test_batch):\n",
    "            y = np.reshape(test_labels,(test_labels.shape[0],1))\n",
    "            batch_x1, batch_x2, batch_y =next_batch(step*batch_size, (step+1)*batch_size, test_pairs, test_labels)\n",
    "            predict, loss_b, summary = sess.run([distance, loss, summ], feed_dict={\n",
    "                                                    im_1: batch_x1[...,None],\n",
    "                                                    im_2: batch_x2[...,None],\n",
    "                                                    labels: batch_y,\n",
    "                                                    is_train: False})\n",
    "            writer.add_summary(summary, epoch*test_batch+step)\n",
    "            acc = compute_accuracy(predict, batch_y)\n",
    "            acc_avg += acc\n",
    "            loss_avg += loss_b\n",
    "        acc_avg /= test_batch\n",
    "        loss_avg /= test_batch\n",
    "        duration = time.time() - start_time\n",
    "        print ('Validation: epoch [%d/%d]: loss: %.3f accuracy: %.3f, duration: %.3f' % (epoch,n_epoch, loss_avg, acc_avg, duration))\n",
    "print('Training completed ...')\n",
    "saver.save(sess, os.path.join(LOGDIR, 'triplet.ckpt'))\n",
    "embed = sess.run(model1_embed, feed_dict= {im_1: X_test[...,None], is_train: False})\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_x1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the embeddings using t-sne\n",
    "http://stackoverflow.com/questions/38189119/simple-way-to-visualize-a-tensorflow-graph-in-jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "display_only = 2000\n",
    "tsne = TSNE(n_components=2)\n",
    "reduced_dim = tsne.fit_transform(embed[:display_only])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_scatter(values, cls):\n",
    "    # Create a color-map with a different color for each class.\n",
    "    import matplotlib.cm as cm\n",
    "    cmap = cm.rainbow(np.linspace(0.0, 1.0, n_classes))\n",
    "\n",
    "    # Get the color for each sample.\n",
    "    colors = cmap[cls]\n",
    "\n",
    "    # Extract the x- and y-values.\n",
    "    x = values[:, 0]\n",
    "    y = values[:, 1]\n",
    "\n",
    "    # Plot it.\n",
    "    plt.scatter(x, y, color=colors)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(reduced_dim, y_test[:display_only])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "def generate_embeddings(embed):\n",
    "    sess= tf.InteractiveSession()\n",
    "    writer = tf.summary.FileWriter(LOGDIR + '/projector', sess.graph)\n",
    "    embedding = tf.Variable(embed[:1024], trainable=False, name='embedding')\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "       \n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    \n",
    "    with open(LOGDIR +'projector/metadata.tsv', 'w') as f:\n",
    "        for label in mnist.test.labels[:1024]:\n",
    "            f.write('{}\\n'.format(label))\n",
    "    embedding_config.metadata_path = LOGDIR +'projector/metadata.tsv'\n",
    "    embedding_config.sprite.image_path = 'mnist_10k_sprite.png'\n",
    "    \n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    projector.visualize_embeddings(writer, config)\n",
    "    \n",
    "    saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), 1)\n",
    "    show_graph(tf.get_default_graph().as_graph_def())\n",
    "    sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
