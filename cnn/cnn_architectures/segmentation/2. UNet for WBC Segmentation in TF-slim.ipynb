{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet for WBC Segmentation in TF-slim\n",
    "__________________________________\n",
    "> Contains the implementation of UNet using TF-Slim framework to demarcate the boundary of WBC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# load the necessary\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import math as m\n",
    "import sys\n",
    "print('Python version:',sys.version)\n",
    "import tensorflow as tf\n",
    "print('TF version:',tf.__version__)\n",
    "from data_utils import data_augmentation \n",
    "slim = tf.contrib.slim\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## Architecture\n",
    "\n",
    "> 1. The model is implemented in tensorflow Slim framework for better readability.   \n",
    "> 2. The preprocessing pipeline is done using the dataset API, where dynamic switching between test and validation sets are implemented.  \n",
    "> 3. The number of layers, and feature length of each of the Unet layers are configurable.  \n",
    "> 4. For regularization, dropout is implemented.\n",
    "> 5. For better training convergence, batch norm is implemented after every convolutional layer. \n",
    "> 6. Based on the article https://distill.pub/2016/deconv-checkerboard/ i have used their recommendation of using image resize, instead of transposed convolution. Both these implementations are configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "T_CONV = False\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Encoder block\n",
    "def encoder_block(input_layer, output_channels, is_train, scope):\n",
    "    \"\"\" Encode block on the down size of Unet\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=None):\n",
    "         with tf.contrib.framework.arg_scope([slim.conv2d], \n",
    "                                             normalizer_fn=slim.batch_norm,\n",
    "                                             normalizer_params={\"is_training\":is_train, 'updates_collections': None},\n",
    "                                             padding='SAME',\n",
    "                                             kernel_size=(3,3)):\n",
    "                #Default activation is relu, use slim.repeat to repeat the conv layer twice\n",
    "                conv = slim.repeat(input_layer,2, slim.conv2d, output_channels, scope='conv')\n",
    "                drop = slim.dropout(conv,keep_prob=KEEP_PROB, is_training=is_train)\n",
    "                pool = slim.max_pool2d(drop, kernel_size=[2,2],padding='SAME',scope='pool')\n",
    "                \n",
    "                #print('conv:',drop.get_shape().as_list())\n",
    "                #print('pool:',pool.get_shape().as_list())\n",
    "                return drop, pool\n",
    "# Decoder block\n",
    "def decoder_block(input_layer, concat_layer, output_channels, is_train, scope):\n",
    "    \"\"\" Decode block on the down size of Unet\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=None):\n",
    "         with tf.contrib.framework.arg_scope([slim.conv2d],                                             \n",
    "                                             normalizer_fn=slim.batch_norm,\n",
    "                                             normalizer_params={\"is_training\":is_train,'updates_collections': None},\n",
    "                                             padding='SAME', kernel_size=(3,3)):\n",
    "                \n",
    "                #Found image resize works better than conv2d_transpose\n",
    "                \n",
    "                if T_CONV:\n",
    "                    conv_t = slim.conv2d_transpose(input_layer, output_channels,kernel_size=[2,2], stride=2, scope='up_conv')\n",
    "                else:\n",
    "                    # upsample\n",
    "                    conv_t = tf.image.resize_nearest_neighbor(input_layer, (tf.shape(input_layer)[1]*2, tf.shape(input_layer)[2]*2), name='up_sample')\n",
    "                \n",
    "                #concat the layers                    \n",
    "                concat = tf.concat([conv_t, concat_layer],3)\n",
    "                # Use the repeat function to repeat a layer\n",
    "                conv = slim.repeat(concat,2, slim.conv2d, output_channels//2, scope='conv')\n",
    "                \n",
    "                # droupout for regulatization\n",
    "                drop  = slim.dropout(conv,keep_prob=KEEP_PROB,is_training=is_train)\n",
    "                \n",
    "                #print('conv_t:',conv_t.get_shape().as_list())\n",
    "                #print('concat:',concat.get_shape().as_list())\n",
    "                #print('conv:',drop.get_shape().as_list())\n",
    "                return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet implementation\n",
    "\n",
    "def Unet (inputs, is_train):\n",
    "    \"\"\" Unet implementation\n",
    "    Args:\n",
    "        inputs : placeholder for inputs [batch, HEIGHT, WIDTH, 3]\n",
    "        is_train: tf.bool set to True during training, False otherwise\n",
    "    Returns:\n",
    "        Output tensor same WIDTH and HEIGHT as the input\n",
    "        returns the enc and decoder block outputs as well.\n",
    "    \"\"\"\n",
    "    layers = 4\n",
    "    feature_multiple = 64 #32\n",
    "    \n",
    "    # down layers\n",
    "    enc_convs = []\n",
    "    input_layer = inputs    \n",
    "    for layer in range(0, layers):\n",
    "        features = 2** layer * 32\n",
    "        conv, input_layer = encoder_block(input_layer, features, is_train, 'enc_block'+str(layer+1))       \n",
    "        enc_convs.append(conv)\n",
    "\n",
    "    #up layers\n",
    "    out_layer = input_layer\n",
    "    for layer in range(layers-1, -1, -1):\n",
    "        features = 2**(layer + 1) * feature_multiple\n",
    "        out_layer = decoder_block(out_layer, enc_convs[layer], features, is_train, 'dec_block'+str(layer+1))\n",
    "    \n",
    "    #the final conv 1x1 layer\n",
    "    out_layer = slim.conv2d(out_layer, 2, [1,1], activation_fn=None)\n",
    "        \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_block1/conv/conv_1/weights:0\n",
      "enc_block1/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block1/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block1/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block1/conv/conv_2/weights:0\n",
      "enc_block1/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block1/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block1/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block2/conv/conv_1/weights:0\n",
      "enc_block2/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block2/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block2/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block2/conv/conv_2/weights:0\n",
      "enc_block2/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block2/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block2/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block3/conv/conv_1/weights:0\n",
      "enc_block3/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block3/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block3/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block3/conv/conv_2/weights:0\n",
      "enc_block3/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block3/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block3/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block4/conv/conv_1/weights:0\n",
      "enc_block4/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block4/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block4/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block4/conv/conv_2/weights:0\n",
      "enc_block4/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block4/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block4/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block4/conv/conv_1/weights:0\n",
      "dec_block4/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block4/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block4/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block4/conv/conv_2/weights:0\n",
      "dec_block4/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block4/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block4/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block3/conv/conv_1/weights:0\n",
      "dec_block3/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block3/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block3/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block3/conv/conv_2/weights:0\n",
      "dec_block3/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block3/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block3/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block2/conv/conv_1/weights:0\n",
      "dec_block2/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block2/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block2/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block2/conv/conv_2/weights:0\n",
      "dec_block2/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block2/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block2/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block1/conv/conv_1/weights:0\n",
      "dec_block1/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block1/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block1/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block1/conv/conv_2/weights:0\n",
      "dec_block1/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block1/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block1/conv/conv_2/BatchNorm/moving_variance:0\n",
      "Conv/weights:0\n",
      "Conv/biases:0\n"
     ]
    }
   ],
   "source": [
    "# Check the above graph\n",
    "tf.reset_default_graph()\n",
    "net = Unet(tf.placeholder(tf.float32,(None, 128, 128, 3)), True)\n",
    "for var in tf.global_variables():\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### Create input pipelines for both train and validation\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input pipeline for both test and validation\n",
    "\n",
    "def binary_threshold(x, thres=0.5):\n",
    "    \"\"\" threshold to 0-1\"\"\"\n",
    "    cond = tf.less(x, thres)\n",
    "    out = tf.where(cond, tf.zeros_like(x), tf.ones_like(x))    \n",
    "    return out\n",
    "\n",
    "def _parse_function(line):\n",
    "    \"\"\" parse the csv, read image and mask\"\"\"\n",
    "    image_raw, mask_raw = tf.decode_csv(line, record_defaults=[[\"\"],[\"\"]])\n",
    "    image = tf.image.decode_jpeg(tf.read_file(image_raw))\n",
    "    mask = tf.image.decode_jpeg(tf.read_file(mask_raw))    \n",
    "    mask = tf.cast(binary_threshold(tf.cast(mask, tf.float32)/255.), tf.int32)\n",
    "    return tf.cast(image, tf.float32)/255., mask\n",
    "\n",
    "def _augment(image, mask):\n",
    "    \"\"\"Function that does input augmentation\"\"\"\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    image, mask = data_augmentation(image, mask)\n",
    "    \n",
    "    #binarize mask to 0-1\n",
    "    mask = tf.cast(binary_threshold(mask), tf.int32)\n",
    "    return image, mask\n",
    "\n",
    "def _resize_valid(image, mask):\n",
    "    \"\"\"Resize the images to nearest multiple of 64\n",
    "       Odd sizes will lead to concatination issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    height = tf.truncatediv(tf.shape(image)[0],64)*64\n",
    "    width = tf.truncatediv(tf.shape(image)[1],64)*64\n",
    "    \n",
    "    image = tf.image.resize_images(image, [height, width])\n",
    "    mask = tf.image.resize_images(mask, [height, width])\n",
    "    #binarize mask to 0-1\n",
    "    mask = tf.cast(binary_threshold(mask), tf.int32)\n",
    "    \n",
    "    return image, mask\n",
    "    \n",
    "# The filename file must contain the path for images and the labels\n",
    "def input_pipeline(filename, batch_size, validation=False):\n",
    "    \"\"\" Input data pipeline, no augmentation during validation\"\"\"\n",
    "    # Read from csv\n",
    "    dataset = tf.data.TextLineDataset([filename])\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "    \n",
    "    # shuffle for only train set\n",
    "    if validation == False:\n",
    "        # here iam combining both normal and augmented samples\n",
    "        augmented = dataset.map(_augment, num_parallel_calls=4)\n",
    "        dataset = dataset.concatenate(augmented)\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "    else:\n",
    "        dataset = dataset.map(_resize_valid).shuffle(buffer_size=5)\n",
    "        dataset = dataset.batch(batch_size)\n",
    " \n",
    "    return dataset    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________\n",
    "## Create the graph and running it in a session.\n",
    "> The input pipelines for both train and validation can be dynamically switched.\n",
    "> Since the validation samples are of different size, they are given as single batch one by one.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, axis=None, smooth = 0.001):\n",
    "    if axis is None:\n",
    "        axis=[1,2]\n",
    "    y_true_f = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred_f = tf.cast(y_pred, dtype=tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=axis)\n",
    "    dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f, axis=axis)\n",
    "                                           + tf.reduce_sum(y_pred_f, axis=axis) + smooth)\n",
    "    return -tf.reduce_mean(dice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the graph\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "LOG_FREQ = 100\n",
    "MODEL_DIR = './unet_model'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    #1. Create valid and train iterators\n",
    "    #-------------------------------------------\n",
    "    # A feedable iterator that can be switched between train and valid samples cannot be used in this case\n",
    "    # as the train and valid output sizes are different, so we will use a bool to switch between the two.\n",
    "    \n",
    "    training_filenames = 'train.csv'\n",
    "    validation_filenames = 'validation.csv'\n",
    "    \n",
    "    # Create valid and train iterators\n",
    "    training_dataset = input_pipeline(training_filenames, BATCH_SIZE)\n",
    "    validation_dataset = input_pipeline(validation_filenames, 1,  validation=True)\n",
    "\n",
    "    # A bool to switch between training loop and testing loop\n",
    "    is_train = tf.placeholder(dtype=tf.bool, name='is_train')\n",
    "    \n",
    "    # A feedable iterator is defined by a handle placeholder and its structure.\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, \n",
    "                                                   training_dataset.output_types, \n",
    "                                                   training_dataset.output_shapes)\n",
    "    # Returns a batch of image and mask at every call\n",
    "    image_batch, mask_batch = iterator.get_next()\n",
    "    \n",
    "    # Create a initializable iterator for valid dataset, \n",
    "    # so that the dataset is same for every valid loop.\n",
    "    validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "    training_iterator = training_dataset.make_one_shot_iterator()#make_initializable_iterator()\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # 2. load Unet to the graph\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # Placeholder definitions to handle incase of testing individual images\n",
    "    X = tf.placeholder_with_default(image_batch, shape=[None,None,None,3], name='X')\n",
    "    y = tf.placeholder_with_default(mask_batch, shape=[None,None,None,1], name='y')\n",
    "    \n",
    "    #Unet logits\n",
    "    logits = Unet(X, is_train)   \n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # 3. Loss and accuracy\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    y = tf.squeeze(y)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(y,2), logits=logits)\n",
    "    \n",
    "    solver = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "   \n",
    "    train_op = slim.learning.create_train_op(loss, solver, global_step=global_step)\n",
    "    \n",
    "    # Probabilities of the outputs\n",
    "    prob = tf.nn.softmax(logits, name='prob')\n",
    "    \n",
    "    # Prediction \n",
    "    pred = tf.argmax(prob, 3, name='pred')\n",
    "\n",
    "    #accuracy \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(pred, tf.int32), tf.cast(y, tf.int32)), tf.float32), name='acc')\n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:0:Train Loss:0.70553\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:0:Valid Loss:0.65323:Accuracy:0.98655\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:100:Train Loss:0.16027\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:200:Train Loss:0.13283\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:300:Train Loss:0.12480\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:400:Train Loss:0.10738\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:500:Train Loss:0.13480\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:500:Valid Loss:0.67323:Accuracy:0.51926\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:600:Train Loss:0.08831\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:700:Train Loss:0.12008\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:800:Train Loss:0.08028\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:900:Train Loss:0.10024\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1000:Train Loss:0.10024\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:1000:Valid Loss:0.52609:Accuracy:0.99206\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1100:Train Loss:0.10604\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1200:Train Loss:0.10975\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1300:Train Loss:0.14118\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1400:Train Loss:0.10689\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1500:Train Loss:0.09595\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:1500:Valid Loss:0.40223:Accuracy:0.98833\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1600:Train Loss:0.11360\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1700:Train Loss:0.15148\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1800:Train Loss:0.11029\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:1900:Train Loss:0.12366\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2000:Train Loss:0.07281\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:2000:Valid Loss:0.40947:Accuracy:0.99231\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2100:Train Loss:0.09613\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2200:Train Loss:0.11419\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2300:Train Loss:0.07299\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2400:Train Loss:0.07459\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2500:Train Loss:0.08693\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:2500:Valid Loss:0.33616:Accuracy:0.98905\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2600:Train Loss:0.06629\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2700:Train Loss:0.10165\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2800:Train Loss:0.10951\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:2900:Train Loss:0.05065\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3000:Train Loss:0.17178\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:3000:Valid Loss:0.28707:Accuracy:0.99231\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3100:Train Loss:0.08647\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3200:Train Loss:0.06377\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3300:Train Loss:0.08582\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3400:Train Loss:0.11940\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3500:Train Loss:0.20742\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:3500:Valid Loss:0.27183:Accuracy:0.99211\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3600:Train Loss:0.11268\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3700:Train Loss:0.08485\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3800:Train Loss:0.06098\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:3900:Train Loss:0.11119\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4000:Train Loss:0.12162\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:4000:Valid Loss:0.24473:Accuracy:0.99026\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4100:Train Loss:0.10117\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4200:Train Loss:0.18672\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4300:Train Loss:0.04857\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4400:Train Loss:0.06388\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4500:Train Loss:0.18744\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:4500:Valid Loss:0.23477:Accuracy:0.99263\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4600:Train Loss:0.09413\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4700:Train Loss:0.10444\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4800:Train Loss:0.12427\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:4900:Train Loss:0.10752\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5000:Train Loss:0.08273\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:5000:Valid Loss:0.22096:Accuracy:0.99709\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5100:Train Loss:0.05666\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5200:Train Loss:0.08509\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5300:Train Loss:0.08940\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5400:Train Loss:0.04925\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5500:Train Loss:0.07236\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:5500:Valid Loss:0.26214:Accuracy:0.99695\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5600:Train Loss:0.10728\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5700:Train Loss:0.06138\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5800:Train Loss:0.09560\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:5900:Train Loss:0.11083\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6000:Train Loss:0.08290\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:6000:Valid Loss:0.21063:Accuracy:0.99713\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6100:Train Loss:0.05618\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6200:Train Loss:0.06478\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6300:Train Loss:0.08856\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6400:Train Loss:0.07646\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6500:Train Loss:0.09730\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:6500:Valid Loss:0.19389:Accuracy:0.99757\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6600:Train Loss:0.06633\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6700:Train Loss:0.04844\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6800:Train Loss:0.04702\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:6900:Train Loss:0.06804\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7000:Train Loss:0.03663\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:7000:Valid Loss:0.12969:Accuracy:0.99771\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mSaving the model...\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7100:Train Loss:0.08566\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7200:Train Loss:0.06401\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7300:Train Loss:0.08801\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7400:Train Loss:0.11530\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7500:Train Loss:0.09607\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:7500:Valid Loss:0.15739:Accuracy:0.99765\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7600:Train Loss:0.05930\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7700:Train Loss:0.06998\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7800:Train Loss:0.10784\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:7900:Train Loss:0.09026\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8000:Train Loss:0.08086\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:8000:Valid Loss:0.25018:Accuracy:0.99592\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8100:Train Loss:0.05038\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8200:Train Loss:0.08050\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8300:Train Loss:0.15420\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8400:Train Loss:0.05650\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8500:Train Loss:0.06275\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:8500:Valid Loss:0.21743:Accuracy:0.99797\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8600:Train Loss:0.04261\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8700:Train Loss:0.10463\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8800:Train Loss:0.07979\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:8900:Train Loss:0.04368\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:9000:Train Loss:0.08372\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:9000:Valid Loss:0.18474:Accuracy:0.99797\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[31m\u001b[1mStopping the training...\u001b[39m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Run the graph in the session\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_handle,valid_handle = sess.run([training_iterator.string_handle(),\n",
    "                                                  validation_iterator.string_handle()])\n",
    "\n",
    "    # variables for saving model and early exit\n",
    "    loss_sum = 0.\n",
    "    best_v_loss = 10e10\n",
    "    early_stopping = 0\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            # the train call\n",
    "            _, l, step = sess.run([train_op,loss, global_step], {is_train:True, handle:train_handle} )\n",
    "            if (step %LOG_FREQ) == 0:\n",
    "                    tf.logging.info(Fore.BLUE + Style.BRIGHT +'Step:{}:Train Loss:{:.5f}'.format(step,l)+Fore.RESET+Style.RESET_ALL)\n",
    "                    train_pred, train_mask = sess.run([pred, y], {is_train:True, handle:train_handle} )\n",
    "\n",
    "            # the validation call\n",
    "            if (step %(LOG_FREQ*5)) == 0:\n",
    "                valid_loss = 0.\n",
    "                valid_acc = 0.\n",
    "                count = 0\n",
    "                sess.run(validation_iterator.initializer, {is_train:False})\n",
    "                while True:\n",
    "                    try:\n",
    "                        acc, valid_pred, valid_mask,l = sess.run([accuracy,pred,y,loss], {is_train:False, handle:valid_handle} )\n",
    "\n",
    "                        valid_loss += l\n",
    "                        valid_acc += acc\n",
    "                        count += 1\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                valid_loss = valid_loss/count\n",
    "                tf.logging.info(Fore.GREEN + Style.BRIGHT +'Step:{}:Valid Loss:{:.5f}:Accuracy:{:.5f}'.format(step,valid_loss, valid_acc/count)+Fore.RESET+Style.RESET_ALL)\n",
    "\n",
    "                 # Save the best model based on valid loss\n",
    "                if (best_v_loss > valid_loss) and (step > 0):\n",
    "                    tf.logging.info(Fore.RED+ Style.BRIGHT +'Saving the model...'+Fore.RESET+Style.RESET_ALL)\n",
    "                    saver.save(sess, MODEL_DIR)\n",
    "                    best_v_loss = valid_loss\n",
    "                    early_stopping = 0\n",
    "                else:\n",
    "                    early_stopping += 1\n",
    "\n",
    "                if early_stopping > 3:\n",
    "                    tf.logging.info(Fore.RED+ Style.BRIGHT +'Stopping the training...'+Fore.RESET+Style.RESET_ALL)\n",
    "                    break\n",
    "\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Completed training...')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- EOF -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
