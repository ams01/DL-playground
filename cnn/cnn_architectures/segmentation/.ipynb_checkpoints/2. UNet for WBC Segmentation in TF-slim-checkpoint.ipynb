{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet for WBC Segmentation in TF-slim\n",
    "__________________________________\n",
    "> Contains the implementation of UNet using TF-Slim framework to demarcate the boundary of WBC's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.5.2 (default, Nov 23 2017, 16:37:01) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "# load the necessary\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import math as m\n",
    "import sys\n",
    "print('Python version:',sys.version)\n",
    "import tensorflow as tf\n",
    "print('TF version:',tf.__version__)\n",
    "from data_utils import data_augmentation \n",
    "slim = tf.contrib.slim\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## Architecture\n",
    "\n",
    "> 1. The model is implemented in tensorflow Slim framework for better readability.   \n",
    "> 2. The preprocessing pipeline is done using the dataset API, where dynamic switching between test and validation sets are implemented.  \n",
    "> 3. The number of layers, and feature length of each of the Unet layers are configurable.  \n",
    "> 4. For regularization, dropout is implemented.\n",
    "> 5. For better training convergence, batch norm is implemented after every convolutional layer. \n",
    "> 6. Based on the article https://distill.pub/2016/deconv-checkerboard/ i have used their recommendation of using image resize, instead of transposed convolution. Both these implementations are configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "T_CONV = False\n",
    "KEEP_PROB = 0.7\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Encoder block\n",
    "def encoder_block(input_layer, output_channels, is_train, scope):\n",
    "    \"\"\" Encode block on the down size of Unet\"\"\"\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=None):\n",
    "         with tf.contrib.framework.arg_scope([slim.conv2d], \n",
    "                                             normalizer_fn=slim.batch_norm,\n",
    "                                             normalizer_params={\"is_training\":is_train, 'updates_collections': None},\n",
    "                                             padding='SAME',\n",
    "                                             kernel_size=(3,3)):\n",
    "                #Default activation is relu, use slim.repeat to repeat the conv layer twice\n",
    "                conv = slim.repeat(input_layer,2, slim.conv2d, output_channels, scope='conv')\n",
    "                drop = slim.dropout(conv,keep_prob=KEEP_PROB, is_training=is_train)\n",
    "                pool = slim.max_pool2d(drop, kernel_size=[2,2],padding='SAME',scope='pool')\n",
    "                \n",
    "                #print('conv:',drop.get_shape().as_list())\n",
    "                #print('pool:',pool.get_shape().as_list())\n",
    "                return drop, pool\n",
    "# Decoder block\n",
    "def decoder_block(input_layer, concat_layer, output_channels, is_train, scope):\n",
    "    \"\"\" Decode block on the down size of Unet\"\"\"\n",
    "    with tf.variable_scope(scope, reuse=None):\n",
    "         with tf.contrib.framework.arg_scope([slim.conv2d],                                             \n",
    "                                             normalizer_fn=slim.batch_norm,\n",
    "                                             normalizer_params={\"is_training\":is_train,'updates_collections': None},\n",
    "                                             padding='SAME', kernel_size=(3,3)):\n",
    "                \n",
    "                #Found image resize works better than conv2d_transpose\n",
    "                \n",
    "                if T_CONV:\n",
    "                    conv_t = slim.conv2d_transpose(input_layer, output_channels,kernel_size=[2,2], stride=2, scope='up_conv')\n",
    "                else:\n",
    "                    # upsample\n",
    "                    conv_t = tf.image.resize_nearest_neighbor(input_layer, (tf.shape(input_layer)[1]*2, tf.shape(input_layer)[2]*2), name='up_sample')\n",
    "                \n",
    "                #concat the layers                    \n",
    "                concat = tf.concat([conv_t, concat_layer],3)\n",
    "                # Use the repeat function to repeat a layer\n",
    "                conv = slim.repeat(concat,2, slim.conv2d, output_channels//2, scope='conv')\n",
    "                \n",
    "                # droupout for regulatization\n",
    "                drop  = slim.dropout(conv,keep_prob=KEEP_PROB,is_training=is_train)\n",
    "                \n",
    "                #print('conv_t:',conv_t.get_shape().as_list())\n",
    "                #print('concat:',concat.get_shape().as_list())\n",
    "                #print('conv:',drop.get_shape().as_list())\n",
    "                return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unet implementation\n",
    "\n",
    "def Unet (inputs, is_train):\n",
    "    \"\"\" Unet implementation\n",
    "    Args:\n",
    "        inputs : placeholder for inputs [batch, HEIGHT, WIDTH, 3]\n",
    "        is_train: tf.bool set to True during training, False otherwise\n",
    "    Returns:\n",
    "        Output tensor same WIDTH and HEIGHT as the input\n",
    "        returns the enc and decoder block outputs as well.\n",
    "    \"\"\"\n",
    "    layers = 4\n",
    "    feature_multiple = 64 #32\n",
    "    \n",
    "    # down layers\n",
    "    enc_convs = []\n",
    "    input_layer = inputs    \n",
    "    for layer in range(0, layers):\n",
    "        features = 2** layer * 32\n",
    "        conv, input_layer = encoder_block(input_layer, features, is_train, 'enc_block'+str(layer+1))       \n",
    "        enc_convs.append(conv)\n",
    "\n",
    "    #up layers\n",
    "    out_layer = input_layer\n",
    "    for layer in range(layers-1, -1, -1):\n",
    "        features = 2**(layer + 1) * feature_multiple\n",
    "        out_layer = decoder_block(out_layer, enc_convs[layer], features, is_train, 'dec_block'+str(layer+1))\n",
    "    \n",
    "    #the final conv 1x1 layer\n",
    "    out_layer = slim.conv2d(out_layer, 2, [1,1], activation_fn=None)\n",
    "        \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_block1/conv/conv_1/weights:0\n",
      "enc_block1/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block1/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block1/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block1/conv/conv_2/weights:0\n",
      "enc_block1/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block1/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block1/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block2/conv/conv_1/weights:0\n",
      "enc_block2/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block2/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block2/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block2/conv/conv_2/weights:0\n",
      "enc_block2/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block2/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block2/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block3/conv/conv_1/weights:0\n",
      "enc_block3/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block3/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block3/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block3/conv/conv_2/weights:0\n",
      "enc_block3/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block3/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block3/conv/conv_2/BatchNorm/moving_variance:0\n",
      "enc_block4/conv/conv_1/weights:0\n",
      "enc_block4/conv/conv_1/BatchNorm/beta:0\n",
      "enc_block4/conv/conv_1/BatchNorm/moving_mean:0\n",
      "enc_block4/conv/conv_1/BatchNorm/moving_variance:0\n",
      "enc_block4/conv/conv_2/weights:0\n",
      "enc_block4/conv/conv_2/BatchNorm/beta:0\n",
      "enc_block4/conv/conv_2/BatchNorm/moving_mean:0\n",
      "enc_block4/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block4/conv/conv_1/weights:0\n",
      "dec_block4/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block4/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block4/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block4/conv/conv_2/weights:0\n",
      "dec_block4/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block4/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block4/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block3/conv/conv_1/weights:0\n",
      "dec_block3/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block3/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block3/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block3/conv/conv_2/weights:0\n",
      "dec_block3/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block3/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block3/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block2/conv/conv_1/weights:0\n",
      "dec_block2/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block2/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block2/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block2/conv/conv_2/weights:0\n",
      "dec_block2/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block2/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block2/conv/conv_2/BatchNorm/moving_variance:0\n",
      "dec_block1/conv/conv_1/weights:0\n",
      "dec_block1/conv/conv_1/BatchNorm/beta:0\n",
      "dec_block1/conv/conv_1/BatchNorm/moving_mean:0\n",
      "dec_block1/conv/conv_1/BatchNorm/moving_variance:0\n",
      "dec_block1/conv/conv_2/weights:0\n",
      "dec_block1/conv/conv_2/BatchNorm/beta:0\n",
      "dec_block1/conv/conv_2/BatchNorm/moving_mean:0\n",
      "dec_block1/conv/conv_2/BatchNorm/moving_variance:0\n",
      "Conv/weights:0\n",
      "Conv/biases:0\n"
     ]
    }
   ],
   "source": [
    "# Check the above graph\n",
    "tf.reset_default_graph()\n",
    "net = Unet(tf.placeholder(tf.float32,(None, 128, 128, 3)), True)\n",
    "for var in tf.global_variables():\n",
    "    print(var.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________\n",
    "### Create input pipelines for both train and validation\n",
    "________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input pipeline for both test and validation\n",
    "\n",
    "def binary_threshold(x, thres=0.5):\n",
    "    \"\"\" threshold to 0-1\"\"\"\n",
    "    cond = tf.less(x, thres)\n",
    "    out = tf.where(cond, tf.zeros_like(x), tf.ones_like(x))    \n",
    "    return out\n",
    "\n",
    "def _parse_function(line):\n",
    "    \"\"\" parse the csv, read image and mask\"\"\"\n",
    "    image_raw, mask_raw = tf.decode_csv(line, record_defaults=[[\"\"],[\"\"]])\n",
    "    image = tf.image.decode_jpeg(tf.read_file(image_raw))\n",
    "    mask = tf.image.decode_jpeg(tf.read_file(mask_raw))    \n",
    "    mask = tf.cast(binary_threshold(tf.cast(mask, tf.float32)/255.), tf.int32)\n",
    "    return tf.cast(image, tf.float32)/255., mask\n",
    "\n",
    "def _augment(image, mask):\n",
    "    \"\"\"Function that does input augmentation\"\"\"\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    image, mask = data_augmentation(image, mask)\n",
    "    \n",
    "    #binarize mask to 0-1\n",
    "    mask = tf.cast(binary_threshold(mask), tf.int32)\n",
    "    return image, mask\n",
    "\n",
    "def _resize_valid(image, mask):\n",
    "    \"\"\"Resize the images to nearest multiple of 64\n",
    "       Odd sizes will lead to concatination issues.\n",
    "    \"\"\"\n",
    "    \n",
    "    height = tf.truncatediv(tf.shape(image)[0],64)*64\n",
    "    width = tf.truncatediv(tf.shape(image)[1],64)*64\n",
    "    \n",
    "    image = tf.image.resize_images(image, [height, width])\n",
    "    mask = tf.image.resize_images(mask, [height, width])\n",
    "    #binarize mask to 0-1\n",
    "    mask = tf.cast(binary_threshold(mask), tf.int32)\n",
    "    \n",
    "    return image, mask\n",
    "    \n",
    "# The filename file must contain the path for images and the labels\n",
    "def input_pipeline(filename, batch_size, validation=False):\n",
    "    \"\"\" Input data pipeline, no augmentation during validation\"\"\"\n",
    "    # Read from csv\n",
    "    dataset = tf.data.TextLineDataset([filename])\n",
    "    dataset = dataset.map(_parse_function, num_parallel_calls=4)\n",
    "    \n",
    "    # shuffle for only train set\n",
    "    if validation == False:\n",
    "        # here iam combining both normal and augmented samples\n",
    "        augmented = dataset.map(_augment, num_parallel_calls=4)\n",
    "        dataset = dataset.concatenate(augmented)\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "    else:\n",
    "        dataset = dataset.map(_resize_valid).shuffle(buffer_size=5)\n",
    "        dataset = dataset.batch(batch_size)\n",
    " \n",
    "    return dataset    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________\n",
    "## Create the graph and running it in a session.\n",
    "> The input pipelines for both train and validation can be dynamically switched.\n",
    "> Since the validation samples are of different size, they are given as single batch one by one.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, axis=None, smooth = 0.001):\n",
    "    if axis is None:\n",
    "        axis=[1,2]\n",
    "    y_true_f = tf.cast(y_true, dtype=tf.float32)\n",
    "    y_pred_f = tf.cast(y_pred, dtype=tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=axis)\n",
    "    dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f, axis=axis)\n",
    "                                           + tf.reduce_sum(y_pred_f, axis=axis) + smooth)\n",
    "    return -tf.reduce_mean(dice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the graph\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 3e-5\n",
    "INPUT_WIDTH = 128\n",
    "INPUT_HEIGHT = 128\n",
    "LOG_FREQ = 100\n",
    "MODEL_DIR = './unet_model'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    #-------------------------------------------\n",
    "    #1. Create valid and train iterators\n",
    "    #-------------------------------------------\n",
    "    # A feedable iterator that can be switched between train and valid samples cannot be used in this case\n",
    "    # as the train and valid output sizes are different, so we will use a bool to switch between the two.\n",
    "    \n",
    "    training_filenames = 'train.csv'\n",
    "    validation_filenames = 'validation.csv'\n",
    "    \n",
    "    # Create valid and train iterators\n",
    "    training_dataset = input_pipeline(training_filenames, BATCH_SIZE)\n",
    "    validation_dataset = input_pipeline(validation_filenames, 1,  validation=True)\n",
    "\n",
    "    # A bool to switch between training loop and testing loop\n",
    "    is_train = tf.placeholder(dtype=tf.bool, name='is_train')\n",
    "    \n",
    "    # A feedable iterator is defined by a handle placeholder and its structure.\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, \n",
    "                                                   training_dataset.output_types, \n",
    "                                                   training_dataset.output_shapes)\n",
    "    # Returns a batch of image and mask at every call\n",
    "    image_batch, mask_batch = iterator.get_next()\n",
    "    \n",
    "    # Create a initializable iterator for valid dataset, \n",
    "    # so that the dataset is same for every valid loop.\n",
    "    validation_iterator = validation_dataset.make_initializable_iterator()\n",
    "    training_iterator = training_dataset.make_one_shot_iterator()#make_initializable_iterator()\n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # 2. load Unet to the graph\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    # Placeholder definitions to handle incase of testing individual images\n",
    "    X = tf.placeholder_with_default(image_batch, shape=[None,None,None,3], name='X')\n",
    "    y = tf.placeholder_with_default(mask_batch, shape=[None,None,None,1], name='y')\n",
    "    \n",
    "    #Unet logits\n",
    "    logits = Unet(X, is_train)   \n",
    "    \n",
    "    #------------------------------------------------\n",
    "    # 3. Loss and accuracy\n",
    "    #------------------------------------------------\n",
    "    \n",
    "    y = tf.squeeze(y)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=tf.one_hot(y,2), logits=logits)\n",
    "    \n",
    "    solver = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "   \n",
    "    train_op = slim.learning.create_train_op(loss, solver, global_step=global_step)\n",
    "    \n",
    "    # Probabilities of the outputs\n",
    "    prob = tf.nn.softmax(logits, name='prob')\n",
    "    \n",
    "    # Prediction \n",
    "    pred = tf.argmax(prob, 3, name='pred')\n",
    "\n",
    "    #accuracy \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(pred, tf.int32), tf.cast(y, tf.int32)), tf.float32), name='acc')\n",
    "                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:\u001b[34m\u001b[1mStep:0:Train Loss:0.86902\u001b[39m\u001b[0m\n",
      "INFO:tensorflow:\u001b[32m\u001b[1mStep:0:Valid Loss:0.67336:Accuracy:0.97495\u001b[39m\u001b[0m\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dec_block1/Dropout/cond/dropout/mul-1-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dec_block1/Dropout/cond/dropout/Floor, PermConstNHWCToNCHW-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: softmax_cross_entropy_loss/value/_213 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3705_softmax_cross_entropy_loss/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dec_block1/Dropout/cond/dropout/mul-1-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dec_block1/Dropout/cond/dropout/Floor, PermConstNHWCToNCHW-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: softmax_cross_entropy_loss/value/_213 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3705_softmax_cross_entropy_loss/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d445acc6dc7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# the train call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_handle\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0mLOG_FREQ\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBLUE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBRIGHT\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'Step:{}:Train Loss:{:.5f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mFore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESET\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mStyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESET_ALL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,64,128,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: dec_block1/Dropout/cond/dropout/mul-1-TransposeNHWCToNCHW-LayoutOptimizer = Transpose[T=DT_FLOAT, Tperm=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](dec_block1/Dropout/cond/dropout/Floor, PermConstNHWCToNCHW-LayoutOptimizer)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: softmax_cross_entropy_loss/value/_213 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3705_softmax_cross_entropy_loss/value\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Run the graph in the session\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    train_handle,valid_handle = sess.run([training_iterator.string_handle(),\n",
    "                                                  validation_iterator.string_handle()])\n",
    "\n",
    "    # variables for saving model and early exit\n",
    "    loss_sum = 0.\n",
    "    best_v_loss = 10e10\n",
    "    early_stopping = 0\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "\n",
    "            # the train call\n",
    "            _, l, step = sess.run([train_op,loss, global_step], {is_train:True, handle:train_handle} )\n",
    "            if (step %LOG_FREQ) == 0:\n",
    "                    tf.logging.info(Fore.BLUE + Style.BRIGHT +'Step:{}:Train Loss:{:.5f}'.format(step,l)+Fore.RESET+Style.RESET_ALL)\n",
    "                    train_pred, train_mask = sess.run([pred, y], {is_train:True, handle:train_handle} )\n",
    "\n",
    "            # the validation call\n",
    "            if (step %(LOG_FREQ*5)) == 0:\n",
    "                valid_loss = 0.\n",
    "                valid_acc = 0.\n",
    "                count = 0\n",
    "                sess.run(validation_iterator.initializer, {is_train:False})\n",
    "                while True:\n",
    "                    try:\n",
    "                        acc, valid_pred, valid_mask,l = sess.run([accuracy,pred,y,loss], {is_train:False, handle:valid_handle} )\n",
    "\n",
    "                        valid_loss += l\n",
    "                        valid_acc += acc\n",
    "                        count += 1\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                valid_loss = valid_loss/count\n",
    "                tf.logging.info(Fore.GREEN + Style.BRIGHT +'Step:{}:Valid Loss:{:.5f}:Accuracy:{:.5f}'.format(step,valid_loss, valid_acc/count)+Fore.RESET+Style.RESET_ALL)\n",
    "\n",
    "                 # Save the best model based on valid loss\n",
    "                if (best_v_loss > valid_loss) and (step > 0):\n",
    "                    tf.logging.info(Fore.RED+ Style.BRIGHT +'Saving the model...'+Fore.RESET+Style.RESET_ALL)\n",
    "                    saver.save(sess, MODEL_DIR)\n",
    "                    best_v_loss = valid_loss\n",
    "                    early_stopping = 0\n",
    "                else:\n",
    "                    early_stopping += 1\n",
    "\n",
    "                if early_stopping > 3:\n",
    "                    tf.logging.info(Fore.RED+ Style.BRIGHT +'Stopping the training...'+Fore.RESET+Style.RESET_ALL)\n",
    "                    break\n",
    "\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Completed training...')\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----- EOF -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
